{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n#### In this notebook, I have taken 100 random samples from various character token ranges. The sample dataset is available [here](https://kaggle.com/datasets/522b8a01db8c65fdc0e5ad2c381fab2f6e3eac5c299d7ae5b99678f9f1987ae6). For each token range, I calculated the ROUGE-1 and ROUGE-L scores. The score dataset is [here](https://kaggle.com/datasets/5d06d02e6778b7d73c2dd67b30db407e4247ba80e69f9d96f974ea94b05064eb).","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y\n!pip install unsloth\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset('csebuetnlp/xlsum', 'bengali',split='test')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sampling\n\nlength_ranges = [\n    (442, 1060), (1060, 1670), (1670, 2280), (2280, 2900),\n    (2900, 3510), (3510, 4130), (4130, 4740), (4740, 5350),\n    (5350, 5920), (5920, 6580)\n]\n\nfor i, (min_len, max_len) in enumerate(length_ranges, start=1):\n    filtered_dataset = dataset.filter(lambda x: isinstance(x['text'], str) and min_len <= len(x['text']) <= max_len)\n    \n    selected_columns = filtered_dataset.remove_columns(\n        [col for col in filtered_dataset.column_names if col not in ['text', 'summary']]\n    )\n    \n    selected_columns = selected_columns.map(lambda x: {'length': f'{min_len}-{max_len}'})\n    \n    random_sample = selected_columns.shuffle(seed=1).select(range(10))\n    save_directory = f\"/kaggle/working/random_sample_{i}\"\n    random_sample.save_to_disk(save_directory)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Rouge1\n\ndef rouge_n_score(candidate, reference, n=1):\n    # Helper function to get n-grams\n    def get_ngrams(text, n):\n        tokens = text.split()\n        return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n\n    # Get n-grams for both candidate and reference\n    candidate_ngrams = get_ngrams(candidate, n)\n    reference_ngrams = get_ngrams(reference, n)\n\n    # Count overlapping n-grams\n    overlap = set(candidate_ngrams) & set(reference_ngrams)\n    overlap_count = len(overlap)\n\n    # Calculate precision, recall, and F1-score\n    precision = overlap_count / len(candidate_ngrams) if candidate_ngrams else 0\n    recall = overlap_count / len(reference_ngrams) if reference_ngrams else 0\n    f1_score = (2 * precision * recall) / (precision + recall) if precision + recall > 0 else 0\n\n    return {\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1_score,\n        'overlap_count': overlap_count\n    }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#RougeL\n\ndef lcs_length(x, y):\n    \"\"\"\n    Helper function to compute the length of the Longest Common Subsequence (LCS)\n    between two sequences x and y using dynamic programming.\n    \n    Args:\n        x: A list of words (tokenized sentence).\n        y: A list of words (tokenized sentence).\n        \n    Returns:\n        Length of the LCS.\n    \"\"\"\n    m = len(x)\n    n = len(y)\n    # Create a 2D DP table to store lengths of LCS\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    \n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if x[i - 1] == y[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1] + 1\n            else:\n                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n    \n    return dp[m][n]\n\ndef rouge_l(candidate, reference):\n    \"\"\"\n    Calculate the ROUGE-L score for a candidate summary and reference summary.\n    \n    Args:\n        candidate: The generated summary (tokenized).\n        reference: The reference summary (tokenized).\n        \n    Returns:\n        Precision, Recall, and F1 score.\n    \"\"\"\n    # Get the length of the longest common subsequence\n    lcs_len = lcs_length(candidate, reference)\n    \n    # Precision\n    precision = lcs_len / len(candidate) if candidate else 0\n    \n    # Recall\n    recall = lcs_len / len(reference) if reference else 0\n    \n    # F1 score\n    if precision + recall == 0:\n        f1_score = 0\n    else:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n    \n    return precision, recall, f1_score\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_seq_length = 4096 \ndtype = None \nload_in_4bit = True\nfrom unsloth import FastLanguageModel\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"hasnatz/gemma2b11\", \n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    \n)\n\nFastLanguageModel.for_inference(model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpaca_prompt = \"\"\"You are an abstructive text summarizer in Bangla language. Below is an instruction in Bangla, paired with an input that provides the context for summarization.\n\n### Instruction:\nনিচের লেখাটির সারমর্ম লিখো\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function for getting all the rouge1 and L result for each sample size.\n\nimport pandas as pd\nfrom tqdm import tqdm  \n\n# Initialize a list to store results\nresults1 = []\nresults2 = []\n\n\n# List of dataset names\ndataset_names = [f'random_sample_{i}' for i in range(1, 11)]\n\n# Iterate through each dataset\nfor dataset_name in dataset_names:\n    random_sample = globals()[dataset_name] \n    \n    total_rouge_1_precision = 0\n    total_rouge_1_recall = 0\n    total_rouge_1_f1 = 0\n\n    total_rouge_l_precision = 0\n    total_rouge_l_recall = 0\n    total_rouge_l_f1 = 0\n    num_samples = len(random_sample['text'])  # Number of samples\n\n    # Iterate through the indices of the 'text' and 'summary' lists\n    for i in tqdm(range(num_samples)):\n        txt = random_sample['text'][i]  # Get the 'text' at index i\n        reference_summary = random_sample['summary'][i]  # Get the reference 'summary' at index i\n\n        # Prepare the inputs for the model\n        inputs = tokenizer(\n            [\n                alpaca_prompt.format(\n                    txt,  # input\n                    \"\",  # output - leave this blank for generation!\n                )\n            ], return_tensors=\"pt\").to(\"cuda\")\n\n        # Generate summary\n        outputs = model.generate(**inputs, max_new_tokens=250, use_cache=True)\n        decoded_outputs = tokenizer.batch_decode(outputs)\n\n        # Extract the generated summary\n        text = decoded_outputs[0]\n\n        # Extracting the response text\n        start_marker = \"### Response:\\n\"\n        end_marker = \"<|end_of_text|>\"\n\n        start_index = text.find(start_marker) + len(start_marker)\n        end_index = text.find(end_marker)\n\n        # Extract the response text\n        candidate_summary = text[start_index:end_index].strip()\n\n        # Calculate ROUGE-1 scores for the current sample\n        rouge_1 = rouge_n_score(candidate_summary, reference_summary, n=1)\n        total_rouge_1_precision += rouge_1['precision']\n        total_rouge_1_recall += rouge_1['recall']\n        total_rouge_1_f1 += rouge_1['f1_score']\n\n        # Calculate ROUGE-L scores for the current sample\n        precision, recall, f1_score = rouge_l(candidate_summary, reference_summary)\n        total_rouge_l_precision += precision\n        total_rouge_l_recall += recall\n        total_rouge_l_f1 += f1_score\n\n    # Calculate average ROUGE-1 scores\n    avg_rouge_1_precision = total_rouge_1_precision / num_samples\n    avg_rouge_1_recall = total_rouge_1_recall / num_samples\n    avg_rouge_1_f1 = total_rouge_1_f1 / num_samples\n\n    # Calculate average ROUGE-L scores\n    avg_rouge_l_precision = total_rouge_l_precision / num_samples\n    avg_rouge_l_recall = total_rouge_l_recall / num_samples\n    avg_rouge_l_f1 = total_rouge_l_f1 / num_samples\n\n    # Create a dictionary to store the results\n    result_dict1 = {\n        \"Model\": \"Gemma29b\",\n        \"Context size\": random_sample['length'],  # Assuming 'length' exists in your dataset\n        \"R1 Precision\": avg_rouge_1_precision,\n        \"R1 Recall\": avg_rouge_1_recall,\n        \"R1 F1\": avg_rouge_1_f1,\n        \n    }\n    \n    result_dict2 ={\n        \"Model\": \"Gemma29b\",\n        \"Context size\": random_sample['length'],\n        \"R-L Precision\": avg_rouge_l_precision,\n        \"R-L Recall\": avg_rouge_l_recall,\n        \"R-L F1\": avg_rouge_l_f1,\n    }\n\n    # Append the result to the results list\n    results1.append(result_dict1)\n    results2.append(result_dict2)\n\n# Convert the results to a DataFrame\nresults_df1 = pd.DataFrame(results1)\nresults_df2 = pd.DataFrame(results2)\n\n\n\nresults_df1.to_csv(\"gm29b_rouge1_results.csv\", index=False)\nresults_df2.to_csv(\"gm29b_rougeL_results.csv\", index=False)\n","metadata":{},"execution_count":null,"outputs":[]}]}